<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>super_gradients.training.models.ddrnet &mdash; SuperGradients 1.0 documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> SuperGradients
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../intro.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../super_gradients.training.html">super_gradients.training package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">SuperGradients</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>super_gradients.training.models.ddrnet</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for super_gradients.training.models.ddrnet</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">from</span> <span class="nn">super_gradients.training.models</span> <span class="kn">import</span> <span class="n">BasicBlock</span><span class="p">,</span> <span class="n">Bottleneck</span><span class="p">,</span> <span class="n">SgModule</span><span class="p">,</span> <span class="n">HpmStruct</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">paper: Deep Dual-resolution Networks for Real-time and</span>
<span class="sd">Accurate Semantic Segmentation of Road Scenes ( https://arxiv.org/pdf/2101.06085.pdf )</span>
<span class="sd">code from git repo: https://github.com/ydhongHIT/DDRNet</span>
<span class="sd">&quot;&quot;&quot;</span>


<div class="viewcode-block" id="ConvBN"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.ConvBN">[docs]</a><span class="k">def</span> <span class="nf">ConvBN</span><span class="p">(</span><span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">add_relu</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">),</span>
           <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">add_relu</span><span class="p">:</span>
        <span class="n">seq</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">seq</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">in_planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">expansion</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">final_relu</span><span class="o">=</span><span class="n">num_blocks</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="n">expansion</span><span class="o">=</span><span class="n">expansion</span><span class="p">))</span>
    <span class="n">in_planes</span> <span class="o">=</span> <span class="n">planes</span> <span class="o">*</span> <span class="n">expansion</span>
    <span class="k">if</span> <span class="n">num_blocks</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">final_relu</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">expansion</span><span class="o">=</span><span class="n">expansion</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">final_relu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">expansion</span><span class="o">=</span><span class="n">expansion</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>


<div class="viewcode-block" id="DAPPMBranch"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DAPPMBranch">[docs]</a><span class="k">class</span> <span class="nc">DAPPMBranch</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">in_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">branch_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">inter_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A DAPPM branch</span>
<span class="sd">        :param kernel_size: the kernel size for the average pooling</span>
<span class="sd">                when stride=0 this parameter is omitted and AdaptiveAvgPool2d over all the input is performed</span>
<span class="sd">        :param stride: stride for the average pooling</span>
<span class="sd">                when stride=0: an AdaptiveAvgPool2d over all the input is performed (output is 1x1)</span>
<span class="sd">                when stride=1: no average pooling is performed</span>
<span class="sd">                when stride&gt;1: average polling is performed (scaling the input down and up again)</span>
<span class="sd">        :param in_planes:</span>
<span class="sd">        :param branch_planes: width after the the first convolution</span>
<span class="sd">        :param inter_mode: interpolation mode for upscaling</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">down_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># when stride is 0 average pool all the input to 1x1</span>
            <span class="n">down_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">elif</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># when stride id 1 no average pooling is used</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">down_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">stride</span><span class="p">))</span>

        <span class="n">down_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">))</span>
        <span class="n">down_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">down_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">branch_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">down_scale</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">down_list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_scale</span> <span class="o">=</span> <span class="n">UpscaleOnline</span><span class="p">(</span><span class="n">inter_mode</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">branch_planes</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">branch_planes</span><span class="p">,</span> <span class="n">branch_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="p">)</span>

<div class="viewcode-block" id="DAPPMBranch.forward"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DAPPMBranch.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        All branches of the DAPPM but the first one receive the output of the previous branch as a second input</span>
<span class="sd">        :param x: in branch 0 - the original input of the DAPPM. in other branches - a list containing the original</span>
<span class="sd">        input and the output of the previous branch.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">output_of_prev_branch</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_of_prev_branch</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">in_width</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">in_height</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">down_scale</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up_scale</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">output_height</span><span class="o">=</span><span class="n">in_height</span><span class="p">,</span> <span class="n">output_width</span><span class="o">=</span><span class="n">in_width</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_of_prev_branch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">out</span> <span class="o">+</span> <span class="n">output_of_prev_branch</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span></div></div>


<div class="viewcode-block" id="DAPPM"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DAPPM">[docs]</a><span class="k">class</span> <span class="nc">DAPPM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">branch_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">kernel_sizes</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">strides</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">inter_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_sizes</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">strides</span><span class="p">),</span> <span class="s1">&#39;len of kernel_sizes and strides must be the same&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">branches</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">kernel_sizes</span><span class="p">,</span> <span class="n">strides</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">branches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">DAPPMBranch</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                             <span class="n">in_planes</span><span class="o">=</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">branch_planes</span><span class="o">=</span><span class="n">branch_planes</span><span class="p">,</span> <span class="n">inter_mode</span><span class="o">=</span><span class="n">inter_mode</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">compression</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">branch_planes</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">branches</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">branch_planes</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">branches</span><span class="p">),</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="p">)</span>

<div class="viewcode-block" id="DAPPM.forward"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DAPPM.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">branch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">branches</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">x_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">branch</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">branch</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x_list</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]]))</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compression</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">x_list</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span></div></div>


<div class="viewcode-block" id="SegmentHead"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.SegmentHead">[docs]</a><span class="k">class</span> <span class="nc">SegmentHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">inter_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">inter_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Last stage of the segmentation network.</span>
<span class="sd">        Reduces the number of output planes (usually to num_classes) while increasing the size by scale_factor</span>
<span class="sd">        :param in_planes: width of input</span>
<span class="sd">        :param inter_planes: width of internal conv. must be a multiple of scale_factor^2 when inter_mode=pixel_shuffle</span>
<span class="sd">        :param out_planes: output width</span>
<span class="sd">        :param scale_factor: scaling factor</span>
<span class="sd">        :param inter_mode: one of nearest, linear, bilinear, bicubic, trilinear, area or pixel_shuffle.</span>
<span class="sd">        when set to pixel_shuffle, an nn.PixelShuffle will be used for scaling</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">inter_mode</span> <span class="o">==</span> <span class="s1">&#39;pixel_shuffle&#39;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">inter_planes</span> <span class="o">%</span> <span class="p">(</span><span class="n">scale_factor</span> <span class="o">^</span> <span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;when using pixel_shuffle, inter_planes must be a multiple of scale_factor^2&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">inter_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">inter_planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inter_mode</span> <span class="o">==</span> <span class="s1">&#39;pixel_shuffle&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">inter_planes</span><span class="p">,</span> <span class="n">inter_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">upscale</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">inter_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">upscale</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">inter_mode</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span> <span class="o">=</span> <span class="n">scale_factor</span>

<div class="viewcode-block" id="SegmentHead.forward"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.SegmentHead.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upscale</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span></div></div>


<div class="viewcode-block" id="UpscaleOnline"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.UpscaleOnline">[docs]</a><span class="k">class</span> <span class="nc">UpscaleOnline</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    In some cases the required scale/size for the scaling is known only when the input is received.</span>
<span class="sd">    This class support such cases. only the interpolation mode is set in advance.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>

<div class="viewcode-block" id="UpscaleOnline.forward"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.UpscaleOnline.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">output_height</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_width</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DDRBackBoneBase"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DDRBackBoneBase">[docs]</a><span class="k">class</span> <span class="nc">DDRBackBoneBase</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A base class defining functions that must be supported by DDRBackBones &quot;&quot;&quot;</span>

<div class="viewcode-block" id="DDRBackBoneBase.validate_backbone_attributes"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DDRBackBoneBase.validate_backbone_attributes">[docs]</a>    <span class="k">def</span> <span class="nf">validate_backbone_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">expected_attributes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;stem&#39;</span><span class="p">,</span> <span class="s1">&#39;layer1&#39;</span><span class="p">,</span> <span class="s1">&#39;layer2&#39;</span><span class="p">,</span> <span class="s1">&#39;layer3&#39;</span><span class="p">,</span> <span class="s1">&#39;layer4&#39;</span><span class="p">,</span> <span class="s1">&#39;input_channels&#39;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">attribute</span> <span class="ow">in</span> <span class="n">expected_attributes</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attribute</span><span class="p">),</span> <span class="sa">f</span><span class="s1">&#39;Invalid backbone - attribute </span><span class="se">\&#39;</span><span class="si">{</span><span class="n">attribute</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1"> is missing&#39;</span></div>

<div class="viewcode-block" id="DDRBackBoneBase.get_backbone_output_number_of_channels"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DDRBackBoneBase.get_backbone_output_number_of_channels">[docs]</a>    <span class="k">def</span> <span class="nf">get_backbone_output_number_of_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return a dictionary of the shapes of each output of the backbone to determine the in_channels of the</span>
<span class="sd">        skip and compress layers&quot;&quot;&quot;</span>
        <span class="n">output_shapes</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_channels</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">320</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output_shapes</span><span class="p">[</span><span class="s1">&#39;layer2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output_shapes</span><span class="p">[</span><span class="s1">&#39;layer3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output_shapes</span><span class="p">[</span><span class="s1">&#39;layer4&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">output_shapes</span></div></div>


<div class="viewcode-block" id="BasicDDRBackBone"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.BasicDDRBackBone">[docs]</a><span class="k">class</span> <span class="nc">BasicDDRBackBone</span><span class="p">(</span><span class="n">DDRBackBoneBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stem</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">ConvBN</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">add_relu</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">ConvBN</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">add_relu</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block</span><span class="p">,</span> <span class="n">in_planes</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">planes</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">num_blocks</span><span class="o">=</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block</span><span class="p">,</span> <span class="n">in_planes</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">planes</span><span class="o">=</span><span class="n">width</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_blocks</span><span class="o">=</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block</span><span class="p">,</span> <span class="n">in_planes</span><span class="o">=</span><span class="n">width</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">planes</span><span class="o">=</span><span class="n">width</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">num_blocks</span><span class="o">=</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block</span><span class="p">,</span> <span class="n">in_planes</span><span class="o">=</span><span class="n">width</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">planes</span><span class="o">=</span><span class="n">width</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="n">num_blocks</span><span class="o">=</span><span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span></div>


<div class="viewcode-block" id="RegnetDDRBackBone"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.RegnetDDRBackBone">[docs]</a><span class="k">class</span> <span class="nc">RegnetDDRBackBone</span><span class="p">(</span><span class="n">DDRBackBoneBase</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Translation of Regnet to fit DDR model</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">regnet_module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="vm">__class__</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_channels</span> <span class="o">=</span> <span class="n">regnet_module</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">stem</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stem</span> <span class="o">=</span> <span class="n">regnet_module</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">stem</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">regnet_module</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">stage_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">regnet_module</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">stage_1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">regnet_module</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">stage_2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="n">regnet_module</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">stage_3</span></div>


<div class="viewcode-block" id="DDRNet"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DDRNet">[docs]</a><span class="k">class</span> <span class="nc">DDRNet</span><span class="p">(</span><span class="n">SgModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">:</span> <span class="n">DDRBackBoneBase</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">additional_layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">upscale_module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
                 <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">highres_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">spp_width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">head_width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">aux_head</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">ssp_inter_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">,</span>
                 <span class="n">segmentation_inter_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">skip_block</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="vm">__class__</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">layer5_block</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="vm">__class__</span> <span class="o">=</span> <span class="n">Bottleneck</span><span class="p">,</span> <span class="n">layer5_bottleneck_expansion</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">classification_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">spp_kernel_sizes</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                 <span class="n">spp_strides</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        :param backbone: the low resolution branch of DDR, expected to have specific attributes in the class</span>
<span class="sd">        :param additional_layers: list of num blocks for the highres stage and layer5</span>
<span class="sd">        :param upscale_module: upscale to use in the backbone (DAPPM and Segmentation head are using bilinear interpolation)</span>
<span class="sd">        :param num_classes: number of classes</span>
<span class="sd">        :param highres_planes: number of channels in the high resolution net</span>
<span class="sd">        :param aux_head: add a second segmentation head (fed from after compress3 + upscale). this head can be used</span>
<span class="sd">        during training (see paper https://arxiv.org/pdf/2101.06085.pdf for details)</span>
<span class="sd">        :param ssp_inter_mode: the interpolation used in the SPP block</span>
<span class="sd">        :param segmentation_inter_mode: the interpolation used in the segmentation head</span>
<span class="sd">        :param skip_block: allows specifying a different block (from &#39;block&#39;) for the skip layer</span>
<span class="sd">        :param layer5_block: type of block to use in layer5 and layer5_skip</span>
<span class="sd">        :param layer5_bottleneck_expansion: determines the expansion rate for Bottleneck block</span>
<span class="sd">        :param spp_kernel_sizes: list of kernel sizes for the spp module pooling</span>
<span class="sd">        :param spp_strides: list of strides for the spp module pooling</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aux_head</span> <span class="o">=</span> <span class="n">aux_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upscale</span> <span class="o">=</span> <span class="n">upscale_module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ssp_inter_mode</span> <span class="o">=</span> <span class="n">ssp_inter_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">segmentation_inter_mode</span> <span class="o">=</span> <span class="n">segmentation_inter_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classification_mode</span> <span class="o">=</span> <span class="n">classification_mode</span>

        <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">aux_head</span> <span class="ow">and</span> <span class="n">classification_mode</span><span class="p">),</span> <span class="s2">&quot;auxiliary head cannot be used in classification mode&quot;</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">DDRBackBoneBase</span><span class="p">),</span> <span class="s1">&#39;The backbone must inherit from AbstractDDRBackBone&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">backbone</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">validate_backbone_attributes</span><span class="p">()</span>
        <span class="n">out_chan_backbone</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">get_backbone_output_number_of_channels</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">compression3</span> <span class="o">=</span> <span class="n">ConvBN</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer3&#39;</span><span class="p">],</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">highres_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                   <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compression4</span> <span class="o">=</span> <span class="n">ConvBN</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer4&#39;</span><span class="p">],</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">highres_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                   <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">down3</span> <span class="o">=</span> <span class="n">ConvBN</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">highres_planes</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer3&#39;</span><span class="p">],</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">down4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">ConvBN</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">highres_planes</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">highres_planes</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                   <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_relu</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">ConvBN</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">highres_planes</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer4&#39;</span><span class="p">],</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                   <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3_skip</span> <span class="o">=</span> <span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">skip_block</span><span class="p">,</span> <span class="n">in_planes</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer2&#39;</span><span class="p">],</span> <span class="n">planes</span><span class="o">=</span><span class="n">highres_planes</span><span class="p">,</span>
                                       <span class="n">num_blocks</span><span class="o">=</span><span class="n">additional_layers</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4_skip</span> <span class="o">=</span> <span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">skip_block</span><span class="p">,</span> <span class="n">in_planes</span><span class="o">=</span><span class="n">highres_planes</span><span class="p">,</span> <span class="n">planes</span><span class="o">=</span><span class="n">highres_planes</span><span class="p">,</span>
                                       <span class="n">num_blocks</span><span class="o">=</span><span class="n">additional_layers</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer5_skip</span> <span class="o">=</span> <span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">layer5_block</span><span class="p">,</span> <span class="n">in_planes</span><span class="o">=</span><span class="n">highres_planes</span><span class="p">,</span> <span class="n">planes</span><span class="o">=</span><span class="n">highres_planes</span><span class="p">,</span>
                                       <span class="n">num_blocks</span><span class="o">=</span><span class="n">additional_layers</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">expansion</span><span class="o">=</span><span class="n">layer5_bottleneck_expansion</span><span class="p">)</span>

        <span class="c1"># when training the backbones on Imagenet:</span>
        <span class="c1">#  - layer 5 has stride 1</span>
        <span class="c1">#  - a new high_to_low_fusion is added with to 3x3 convs with stride 2 (and double the width)</span>
        <span class="c1">#  - a classification head is placed instead of the segmentation head</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">classification_mode</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer5</span> <span class="o">=</span> <span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">layer5_block</span><span class="p">,</span> <span class="n">in_planes</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer4&#39;</span><span class="p">],</span>
                                      <span class="n">planes</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer4&#39;</span><span class="p">],</span> <span class="n">num_blocks</span><span class="o">=</span><span class="n">additional_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                      <span class="n">expansion</span><span class="o">=</span><span class="n">layer5_bottleneck_expansion</span><span class="p">)</span>

            <span class="n">highres_planes_out</span> <span class="o">=</span> <span class="n">highres_planes</span> <span class="o">*</span> <span class="n">layer5_bottleneck_expansion</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">high_to_low_fusion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">ConvBN</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">highres_planes_out</span><span class="p">,</span>
                                                           <span class="n">out_channels</span><span class="o">=</span><span class="n">highres_planes_out</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
                                                           <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                                           <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">add_relu</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                                                    <span class="n">ConvBN</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">highres_planes_out</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
                                                           <span class="n">out_channels</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer4&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">layer5_bottleneck_expansion</span><span class="p">,</span>
                                                           <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                                           <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">add_relu</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">average_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer4&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">layer5_bottleneck_expansion</span><span class="p">,</span>
                                <span class="n">out_features</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer5</span> <span class="o">=</span> <span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">layer5_block</span><span class="p">,</span> <span class="n">in_planes</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer4&#39;</span><span class="p">],</span>
                                      <span class="n">planes</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer4&#39;</span><span class="p">],</span> <span class="n">num_blocks</span><span class="o">=</span><span class="n">additional_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                      <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">expansion</span><span class="o">=</span><span class="n">layer5_bottleneck_expansion</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">spp</span> <span class="o">=</span> <span class="n">DAPPM</span><span class="p">(</span><span class="n">in_planes</span><span class="o">=</span><span class="n">out_chan_backbone</span><span class="p">[</span><span class="s1">&#39;layer4&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">layer5_bottleneck_expansion</span><span class="p">,</span>
                             <span class="n">branch_planes</span><span class="o">=</span><span class="n">spp_width</span><span class="p">,</span> <span class="n">out_planes</span><span class="o">=</span><span class="n">highres_planes</span> <span class="o">*</span> <span class="n">layer5_bottleneck_expansion</span><span class="p">,</span>
                             <span class="n">inter_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ssp_inter_mode</span><span class="p">,</span> <span class="n">kernel_sizes</span><span class="o">=</span><span class="n">spp_kernel_sizes</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">spp_strides</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_head</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">seghead_extra</span> <span class="o">=</span> <span class="n">SegmentHead</span><span class="p">(</span><span class="n">highres_planes</span><span class="p">,</span> <span class="n">head_width</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span>
                                                 <span class="n">inter_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">segmentation_inter_mode</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">SegmentHead</span><span class="p">(</span><span class="n">highres_planes</span> <span class="o">*</span> <span class="n">layer5_bottleneck_expansion</span><span class="p">,</span>
                                           <span class="n">head_width</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">inter_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">segmentation_inter_mode</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<div class="viewcode-block" id="DDRNet.forward"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DDRNet.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">width_output</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">8</span>
        <span class="n">height_output</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">//</span> <span class="mi">8</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out_layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">out_layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out_layer2</span><span class="p">))</span>
        <span class="n">out_layer3_skip</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3_skip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out_layer2</span><span class="p">))</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">out_layer3</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">down3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out_layer3_skip</span><span class="p">))</span>
        <span class="n">x_skip</span> <span class="o">=</span> <span class="n">out_layer3_skip</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">upscale</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compression3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out_layer3</span><span class="p">)),</span> <span class="n">height_output</span><span class="p">,</span> <span class="n">width_output</span><span class="p">)</span>

        <span class="c1"># save for auxiliary head</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_head</span><span class="p">:</span>
            <span class="n">temp</span> <span class="o">=</span> <span class="n">x_skip</span>

        <span class="n">out_layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out_layer4_skip</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4_skip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x_skip</span><span class="p">))</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">out_layer4</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">down4</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out_layer4_skip</span><span class="p">))</span>
        <span class="n">x_skip</span> <span class="o">=</span> <span class="n">out_layer4_skip</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">upscale</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compression4</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out_layer4</span><span class="p">)),</span> <span class="n">height_output</span><span class="p">,</span> <span class="n">width_output</span><span class="p">)</span>

        <span class="n">out_layer5_skip</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer5_skip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x_skip</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">classification_mode</span><span class="p">:</span>
            <span class="n">x_skip</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">high_to_low_fusion</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out_layer5_skip</span><span class="p">))</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer5</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_pool</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">x_skip</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upscale</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer5</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span> <span class="n">height_output</span><span class="p">,</span> <span class="n">width_output</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">out_layer5_skip</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_head</span><span class="p">:</span>
                <span class="n">x_extra</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seghead_extra</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_extra</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="DDRNetCustom"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DDRNetCustom">[docs]</a><span class="k">class</span> <span class="nc">DDRNetCustom</span><span class="p">(</span><span class="n">DDRNet</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">:</span> <span class="n">HpmStruct</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Parse arch_params and translate the parameters to build the original DDRNet architecture &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">backbone</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span>
                         <span class="n">additional_layers</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">additional_layers</span><span class="p">,</span>
                         <span class="n">upscale_module</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">upscale_module</span><span class="p">,</span>
                         <span class="n">num_classes</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
                         <span class="n">highres_planes</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">highres_planes</span><span class="p">,</span>
                         <span class="n">spp_width</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">spp_planes</span><span class="p">,</span>
                         <span class="n">head_width</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">head_planes</span><span class="p">,</span>
                         <span class="n">aux_head</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">aux_head</span><span class="p">,</span>
                         <span class="n">ssp_inter_mode</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">ssp_inter_mode</span><span class="p">,</span>
                         <span class="n">segmentation_inter_mode</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">segmentation_inter_mode</span><span class="p">,</span>
                         <span class="n">skip_block</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">skip_block</span><span class="p">,</span>
                         <span class="n">layer5_block</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">layer5_block</span><span class="p">,</span>
                         <span class="n">layer5_bottleneck_expansion</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">layer5_bottleneck_expansion</span><span class="p">,</span>
                         <span class="n">classification_mode</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">classification_mode</span><span class="p">,</span>
                         <span class="n">spp_kernel_sizes</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">spp_kernel_sizes</span><span class="p">,</span>
                         <span class="n">spp_strides</span><span class="o">=</span><span class="n">arch_params</span><span class="o">.</span><span class="n">spp_strides</span><span class="p">)</span></div>


<span class="n">DEFAULT_DDRNET_23_PARAMS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;input_channels&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;block&quot;</span><span class="p">:</span> <span class="n">BasicBlock</span><span class="p">,</span>
    <span class="s2">&quot;skip_block&quot;</span><span class="p">:</span> <span class="n">BasicBlock</span><span class="p">,</span>
    <span class="s2">&quot;layer5_block&quot;</span><span class="p">:</span> <span class="n">Bottleneck</span><span class="p">,</span>
    <span class="s2">&quot;layer5_bottleneck_expansion&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">&quot;layers&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="s2">&quot;upscale_module&quot;</span><span class="p">:</span> <span class="n">UpscaleOnline</span><span class="p">(),</span>
    <span class="s2">&quot;planes&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;highres_planes&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">&quot;head_planes&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">&quot;aux_head&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;segmentation_inter_mode&quot;</span><span class="p">:</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">,</span>
    <span class="s2">&quot;classification_mode&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;spp_planes&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">&quot;ssp_inter_mode&quot;</span><span class="p">:</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">,</span>
    <span class="s2">&quot;spp_kernel_sizes&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;spp_strides&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">DEFAULT_DDRNET_23_SLIM_PARAMS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="o">**</span><span class="n">DEFAULT_DDRNET_23_PARAMS</span><span class="p">,</span>
    <span class="s2">&quot;planes&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s2">&quot;highres_planes&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;head_planes&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
<span class="p">}</span>


<div class="viewcode-block" id="DDRNet23"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DDRNet23">[docs]</a><span class="k">class</span> <span class="nc">DDRNet23</span><span class="p">(</span><span class="n">DDRNetCustom</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">:</span> <span class="n">HpmStruct</span><span class="p">):</span>
        <span class="n">_arch_params</span> <span class="o">=</span> <span class="n">HpmStruct</span><span class="p">(</span><span class="o">**</span><span class="n">DEFAULT_DDRNET_23_PARAMS</span><span class="p">)</span>
        <span class="n">_arch_params</span><span class="o">.</span><span class="n">override</span><span class="p">(</span><span class="o">**</span><span class="n">arch_params</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
        <span class="c1"># BUILD THE BACKBONE AND INSERT TO THE _arch_params</span>
        <span class="n">backbone_layers</span><span class="p">,</span> <span class="n">_arch_params</span><span class="o">.</span><span class="n">additional_layers</span> <span class="o">=</span> <span class="n">_arch_params</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="n">_arch_params</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
        <span class="n">_arch_params</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">BasicDDRBackBone</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">_arch_params</span><span class="o">.</span><span class="n">block</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">_arch_params</span><span class="o">.</span><span class="n">planes</span><span class="p">,</span>
                                                 <span class="n">layers</span><span class="o">=</span><span class="n">backbone_layers</span><span class="p">,</span>
                                                 <span class="n">input_channels</span><span class="o">=</span><span class="n">_arch_params</span><span class="o">.</span><span class="n">input_channels</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_arch_params</span><span class="p">)</span></div>


<div class="viewcode-block" id="DDRNet23Slim"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.DDRNet23Slim">[docs]</a><span class="k">class</span> <span class="nc">DDRNet23Slim</span><span class="p">(</span><span class="n">DDRNetCustom</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">:</span> <span class="n">HpmStruct</span><span class="p">):</span>
        <span class="n">_arch_params</span> <span class="o">=</span> <span class="n">HpmStruct</span><span class="p">(</span><span class="o">**</span><span class="n">DEFAULT_DDRNET_23_SLIM_PARAMS</span><span class="p">)</span>
        <span class="n">_arch_params</span><span class="o">.</span><span class="n">override</span><span class="p">(</span><span class="o">**</span><span class="n">arch_params</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
        <span class="c1"># BUILD THE BACKBONE AND INSERT TO THE _arch_params</span>
        <span class="n">backbone_layers</span><span class="p">,</span> <span class="n">_arch_params</span><span class="o">.</span><span class="n">additional_layers</span> <span class="o">=</span> <span class="n">_arch_params</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="n">_arch_params</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
        <span class="n">_arch_params</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">BasicDDRBackBone</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">_arch_params</span><span class="o">.</span><span class="n">block</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">_arch_params</span><span class="o">.</span><span class="n">planes</span><span class="p">,</span>
                                                 <span class="n">layers</span><span class="o">=</span><span class="n">backbone_layers</span><span class="p">,</span>
                                                 <span class="n">input_channels</span><span class="o">=</span><span class="n">_arch_params</span><span class="o">.</span><span class="n">input_channels</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_arch_params</span><span class="p">)</span></div>


<div class="viewcode-block" id="AnyBackBoneDDRNet23"><a class="viewcode-back" href="../../../../super_gradients.training.models.html#super_gradients.training.models.ddrnet.AnyBackBoneDDRNet23">[docs]</a><span class="k">class</span> <span class="nc">AnyBackBoneDDRNet23</span><span class="p">(</span><span class="n">DDRNetCustom</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">:</span> <span class="n">HpmStruct</span><span class="p">):</span>
        <span class="n">_arch_params</span> <span class="o">=</span> <span class="n">HpmStruct</span><span class="p">(</span><span class="o">**</span><span class="n">DEFAULT_DDRNET_23_PARAMS</span><span class="p">)</span>
        <span class="n">_arch_params</span><span class="o">.</span><span class="n">override</span><span class="p">(</span><span class="o">**</span><span class="n">arch_params</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">_arch_params</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">_arch_params</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">8</span><span class="p">,</span> \
            <span class="s1">&#39;The length of </span><span class="se">\&#39;</span><span class="s1">arch_params.layers</span><span class="se">\&#39;</span><span class="s1"> must be 4 or 8&#39;</span>
        <span class="c1"># TAKE THE LAST 4 NUMBERS AS THE ADDITIONAL LAYERS SPECIFICATION</span>
        <span class="n">_arch_params</span><span class="o">.</span><span class="n">additional_layers</span> <span class="o">=</span> <span class="n">_arch_params</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span>
        <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">_arch_params</span><span class="p">,</span> <span class="s1">&#39;backbone&#39;</span><span class="p">),</span> <span class="s1">&#39;AnyBackBoneDDRNet_23 requires having a backbone in arch_params&#39;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">_arch_params</span><span class="p">,</span> <span class="s1">&#39;input_channels&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">_arch_params</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">input_channels</span> <span class="o">==</span> <span class="n">_arch_params</span><span class="o">.</span><span class="n">input_channels</span><span class="p">,</span> \
                <span class="s1">&#39;</span><span class="se">\&#39;</span><span class="s1">input_channels</span><span class="se">\&#39;</span><span class="s1"> was given in arch_params with a different value than existing in the backbone&#39;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_arch_params</span><span class="p">)</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, SuperGradients team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>